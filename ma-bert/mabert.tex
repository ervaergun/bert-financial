\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{chapterbib}
\usepackage[sectionbib]{natbib}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}


\title{\vspace{-4cm} MergerBERT: Predicting Merger Targets and Acquirers from Text via Pretrained Language Models}
\author{}
\date{}

\begin{document}

\maketitle

\section{Abstract}

We explore the use of a U.S.firm's SEC filings to predict whether the firm will be
an acquirer or a target of an acquisition within a year of the
filing. Our approach uses transfer learning.

%Our approach uses text regression, in which frequencies of words and phrases in the document are used as
%independent variables in a logistic regression model.
We find that word and phrase features have significant predictive power in models of being an acquirer or a target. In each case, the
best performing models involve a different use of text alongside
standard financial variables.

Our perform outperforms all other methods.

% Using Machine Learning to Analyze Merger Activity
In this paper, I use machine learning techniques in order to use a new dataset to analyze
merger activity - a firmâ€™s annual 10-k SEC statement. In this statement contains description
about firm and firm product that I believe will capture harder to observe variables that could
do well with explaining mergers. I find that the lasso and the ridge regularization techniques
have found not only words that align with previous merger theory, but also some interesting
variables that were not previously considered. Using this new technique, I obtained a predictive
model that yields an R-squared that ranges from 0.02 to 0.07

Our analysis is two-fold. We apply modified BERT to financial text
prediction. We also present analysis similar to ~\cite{moriarty2019}.


\section{Introduction}


~\cite{katsafados2019}


~\cite{katsafados2020}
%Textual information and IPO underpricing: A machine learning approach
This study examines the predictive power of textual information from S-1 filings in
explaining IPO underpricing. Our empirical approach differs from previous research, as we
utilize several machine learning algorithms to predict whether an IPO will be underpriced, or
not. We analyze a large sample of 2,481 U.S. IPOs from 1997 to 2016, and we find that
textual information can effectively complement traditional financial variables in terms of
prediction accuracy. In fact, models that use both textual data and financial variables as
inputs have superior performance compared to models using a single type of input. We
attribute our findings to the fact that textual information can reduce the ex-ante valuation
uncertainty of IPO firms, thus leading to more accurate estimates.

~\cite{routledge2013}

~\cite{linlin2018}


Mergers and acquisitions (M\&As) play a key role in the economy. At the aggregate level,
M\&A transactions represent the main mechanism for consolidation and restructuring within
industries, and their value as a fraction of U.S. GDP is substantial; $5.8\%$ between $1980$ and
$2011$ (according to M\&A volume is computed from SDC Platinum data as the aggregate value of completed acquisitions of
U.S. target companies). At the individual company level, takeovers constitute major investment decisions and
an effective way to discipline ineffcient managers. Given their importance in the economy, it
is no surprise that M\&As have attracted a great deal of attention among researchers|there
is a wide body of theoretical and empirical research surrounding
mergers.


\subsection{Related Work}


Using textual analysis to identify merger participants: Evidence from the U.S. banking industry~\cite{xxx}.

Confining value from neural networks
A sectoral study prediction of takeover targets
in the US technology sector


Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP)
tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word
representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been
widely demonstrated for English using contextualized representations~\cite{dai2015, peters2018, howard2018, radford2018, devlin2019, yang2019}.

%GPT2 Paper
Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically
approached with supervised learning on task specific datasets. We demonstrate that language
models begin to learn these tasks without any explicit supervision when trained on a new dataset
of millions of webpages called WebText. When
conditioned on a document plus questions, the answers generated by the language model reach 55
F1 on the CoQA dataset - matching or exceeding
the performance of 3 out of 4 baseline systems
without using the 127,000+ training examples.
The capacity of the language model is essential
to the success of zero-shot task transfer and increasing it improves performance in a log-linear
fashion across tasks. Our largest model, GPT-2,
is a 1.5B parameter Transformer that achieves
state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting
but still underfits WebText. Samples from the
model reflect these improvements and contain coherent paragraphs of text. These findings suggest
a promising path towards building language processing systems which learn to perform tasks from
their naturally occurring demonstrations~\cite{radford2019} 

%related work
There are previous methods that use Machine learning to predict stock
markets.~\cite{ding2015} proposes a deep learning method for
event-driven stock market prediction. First, events are extracted from news text, and represented as dense vectors,
trained using a novel neural tensor network. Second, a deep convolutional neural network is used to model both short-term and
long-term influences of events on stock price movements.

The use of robo-readers to analyze news texts is an emerging technology trend in computational finance. In recent
research, a substantial effort has been invested to develop sophisticated financial polarity-lexicons that can be used to
investigate how financial sentiments relate to future company performance. However, based on experience from other
fields, where sentiment analysis is commonly applied, it is well-known
that the overall semantic orientation of a sentence may differ from
the prior polarity of individual words.

% Neural network model
Financial risk, defined as the chance to deviate from return expectations, is most commonly measured
with volatility. Due to its value for investment decision making, volatility prediction is probably
among the most important tasks in finance and risk management. Although evidence exists that enriching purely financial models with natural language
information can improve predictions of volatility, this task is still comparably underexplored. We introduce PRoFET, the
first neural model for volatility prediction jointly exploiting both
semantic language representations and a comprehensive set of financial
features. As language data, we use transcripts from quarterly
recurring events, so-called earnings calls; in these calls, the performance of publicly traded companies is summarized and prognosticated by their management. We show that our
proposed architecture, which models verbal context with an attention mechanism, significantly outperforms the previous state-of-the-art and other strong
baselines. Finally, we visualize this attention mechanism on the token-level, thus aiding interpretability and providing a use case of PRoFET as a tool
for investment decision support~\cite{theil2019}.


\bibliographystyle{plain}
\bibliography{mabert}


\end{document}
