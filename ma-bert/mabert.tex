\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{chapterbib}
\usepackage[sectionbib]{natbib}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}


\title{\vspace{-4cm} MergerBERT: Predicting Merger Targets and Acquirers from Text via Pretradined Language Models}
\author{}
\date{}

\begin{document}

\maketitle

\section{Abstract}

\section{Introduction and Related Work}

Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP)
tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word
representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been
widely demonstrated for English using contextualized representations~\cite{dai2015, peters2018, howard2018, radford2018, devlin2019, yang2019}.

%GPT2 Paper
Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically
approached with supervised learning on task specific datasets. We demonstrate that language
models begin to learn these tasks without any explicit supervision when trained on a new dataset
of millions of webpages called WebText. When
conditioned on a document plus questions, the answers generated by the language model reach 55
F1 on the CoQA dataset - matching or exceeding
the performance of 3 out of 4 baseline systems
without using the 127,000+ training examples.
The capacity of the language model is essential
to the success of zero-shot task transfer and increasing it improves performance in a log-linear
fashion across tasks. Our largest model, GPT-2,
is a 1.5B parameter Transformer that achieves
state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting
but still underfits WebText. Samples from the
model reflect these improvements and contain coherent paragraphs of text. These findings suggest
a promising path towards building language processing systems which learn to perform tasks from
their naturally occurring demonstrations~\cite{radford2019} 


%related work
There are previous methods that use Machine learning to predict stock
markets.~\cite{ding2015} proposes a deep learning method for
event-driven stock market prediction. First, events are extracted from news text, and represented as dense vectors,
trained using a novel neural tensor network. Second, a deep convolutional neural network is used to model both short-term and
long-term influences of events on stock price movements.

The use of robo-readers to analyze news texts is an emerging technology trend in computational finance. In recent
research, a substantial effort has been invested to develop sophisticated financial polarity-lexicons that can be used to
investigate how financial sentiments relate to future company performance. However, based on experience from other
fields, where sentiment analysis is commonly applied, it is well-known
that the overall semantic orientation of a sentence may differ from
the prior polarity of individual words.

% Neural network model
Financial risk, defined as the chance to deviate from return expectations, is most commonly measured
with volatility. Due to its value for investment decision making, volatility prediction is probably
among the most important tasks in finance and risk management. Although evidence exists that enriching purely financial models with natural language
information can improve predictions of volatility, this task is still comparably underexplored. We introduce PRoFET, the
first neural model for volatility prediction jointly exploiting both
semantic language representations and a comprehensive set of financial
features. As language data, we use transcripts from quarterly
recurring events, so-called earnings calls; in these calls, the performance of publicly traded companies is summarized and prognosticated by their management. We show that our
proposed architecture, which models verbal context with an attention mechanism, significantly outperforms the previous state-of-the-art and other strong
baselines. Finally, we visualize this attention mechanism on the token-level, thus aiding interpretability and providing a use case of PRoFET as a tool
for investment decision support~\cite{theil2019}.


\bibliographystyle{plain}
\bibliography{mabert}


\end{document}
