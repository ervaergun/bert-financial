\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{chapterbib}
\usepackage[sectionbib]{natbib}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}


\title{\vspace{-4cm} Predicting Financial Sentiment Through Pretrained Language Models}
\author{}
\date{}

\begin{document}

\maketitle

% Candidate Journals
% Quantitative Finance
% Rview of Financial Studies
% Review of Finance
% Mathematical Finance
%Journal of Financial and Quantitative Analysis
%Finance Research Letters
%Review of Quantitative Finance and Accounting


\section{Introduction and Related Work}

Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP)
tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word
representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been
widely demonstrated for English using contextualized representations~\cite{dai2015, peters2018, howard2018, radford2018, devlin2019, yang2019}.

%GPT2 Paper
Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically
approached with supervised learning on task specific datasets. We demonstrate that language
models begin to learn these tasks without any explicit supervision when trained on a new dataset
of millions of webpages called WebText. When
conditioned on a document plus questions, the answers generated by the language model reach 55
F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems
without using the 127,000+ training examples. The capacity of the language model is essential
to the success of zero-shot task transfer and increasing it improves performance in a log-linear
fashion across tasks. Our largest model, GPT-2,
is a 1.5B parameter Transformer that achieves
state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting
but still underfits WebText. Samples from the
model reflect these improvements and contain coherent paragraphs of text. These findings suggest
a promising path towards building language processing systems which learn to perform tasks from
their naturally occurring demonstrations~\cite{radford2019} 

The use of robo-readers to analyze news texts is an emerging technology trend in computational finance. In recent
research, a substantial effort has been invested to develop sophisticated financial polarity-lexicons that can be used to
investigate how financial sentiments relate to future company performance. However, based on experience from other
fields, where sentiment analysis is commonly applied, it is well-known
that the overall semantic orientation of a sentence may differ from
the prior polarity of individual words.


\section{Experiments}

Using BERT models in finance have also been used in the
literature~\cite{araci2019, yang2020}. Other pretrained models have
also been used~\cite{yang2018}.
%FinBERT: A Pretrained Language Model for Financial Communications.

We do not have many financial datasets for financial tasks.

%Value Investor's club as discussed in~\cite{yang2018}.

\subsection{FiQA}

The provided training dataset for WWW ’18~\cite{maia2018} contains a
total of $1174$ examples from news headlines and tweets. Each example
contains the sentence and the sentence snippet associated with the target entity, aspect, and sentiment score. A
sample FiQA entry is shown in~\ref{tab:fiqa_example}. A Level 1 Aspect
label takes on one of $4$ possible labels (Corporate, Economy, Market or Stock), and our Level 2 Aspect label takes on
one of $27$ possible labels (Appointment, Risks, Dividend Policy, Financial, Legal, Volatility, Coverage, Price Action, etc.). The original
dataset contained a small number of multilabel examples, however, we considered this number
too few to train a meaningful multilabel classifier. Thus, we slightly
stray from the original WWW ’18 task for the purpose of this research. Finally, sentiment score takes on a continuous value between $-1$ and $1$ – most negative to most positive.

We can use FiQA dataset for both classification purpose in terms of aspect levels, and regression purpose in terms of sentiment score.

\begin{table}
  \begin{tabular}{|c|c|}
    \hline 
 Sentence & easyJet expects resilient demand to withstand security
            fears. \\ \hline
Aspect Level 1 &  Corporate \\ \hline 
Aspect Level 2 & Risks \\ \hline 
Sentiment Score & 0.165 \\ \hline 
Target & easyJet \\ \hline 
\end{tabular}
\caption{An example entry from FiQA}
\label{tab:fiqa_example}
\end{table}

A large collection of financial reports published annually by publicly-traded companies is employed to conduct our experiments;
moreover, two analytical techniques – regression and ranking methods –
are applied to conduct these analyses~\cite{xxx}.

Reuter's and Bloomberg dataset: Datasets are in Emre's email.
\url{https://github.com/philipperemy/financial-news-dataset}

Reuter's news dataset:
\url{https://github.com/duynht/financial-news-dataset}

\textbf{NLTK's corpus}
\url{https://www.kaggle.com/boldy717/reutersnltk#__sid=js0}

\textbf{Fed Meeting Notes}
\url{https://fraser.stlouisfed.org/title/federal-open-market-committee-meeting-minutes-transcripts-documents-677?browse=2020s}

FOMC Statements Scraper
https://github.com/souljourner/FOMC-Statements-Minutes-Scraper
Some cleaned transcripts
https://github.com/ali-wetrill/FOMCTranscriptAnalysis

Another source for datasets: \url{https://rstudio-pubs-static.s3.amazonaws.com/495650_c9c874694f164fb5948031801079157f.html#3_data}

\url{https://sraf.nd.edu/textual-analysis/resources/#LM%20Sentiment%20Word%20Lists}

Finally, the texts are stemmed using the Porter stemmer when needed.

10K dataset together with volatilities
http://ifs.tuwien.ac.at/~admire/financialvolatility/


% well-written
Recently, unsupervised pre-training of language models on large corpora has significantly improved the performance of many NLP tasks. The
language models are pretained on generic corpora such as Wikipedia. However, sentiment analysis
is a strongly domain dependent task. Financial sector has accumulated large scale of text of financial and business communications. Therefore,
leveraging the success of unsupervised pretraining and large amount of
financial text could potentially benefit wide range of financial applications.

Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we
propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel
techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively,
and the attention weights among words are computed using disentangled
matrices on their contents and relative positions. Second, an enhanced mask decoder is used
to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of
model pre-training and performance of downstream tasks. Compared to RoBERTaLarge, a DeBERTa model trained on half of the training data performs consistently
better on a wide range of NLP tasks, achieving improvements on MNLI.

Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks.

We show that these two techniques significantly improve the efficiency of
model pre-training and performance of downstream tasks. In financial
domain, the same is observed.

BERT and its variants have significantly enhanced word vector representation. Here, we will focus on specific BERT application on financial
datasets.


\textbf{Financial Phrasebank}


http://www.cs.cmu.edu/~ark/10K/data/
Metadata var

10K downloads: https://pypi.org/project/sec-edgar-downloader/. Filing
date is in each text file.

extract MDA and tokenize new files in Noah's website can be used to clean the dataset.
CIK Ticker Mapping: https://www.sec.gov/include/ticker.txt





\textbf{Corporate Reports 10-K \& 10-Q} The most important text data in finance and business communication is corporate report. In the United States,
the Securities Exchange Commission (SEC) mandates all publicly traded companies to file annual
reports, known as Form 10-K, and quarterly reports, known as Form 10-Q. This document provides a comprehensive overview of the company’s
business and financial condition. Laws and regulations prohibit companies from making materially
false or misleading statements in the 10-Ks. The
Form 10-Ks and 10-Qs are publicly available and can be accesses from
SEC website. We obtain 60,490 Form 10-Ks and 142,622
Form 10-Qs of Russell 3000 firms during 1994 and
2019 from SEC website. We only include sections that are textual components, such as Item 1 (Business) in 10-Ks, Item 1A (Risk Factors) in both 10-
Ks and 10-Qs and Item 7 (Managements Discussion and Analysis) in 10-Ks.


\bibliographystyle{plain}
\bibliography{finbert}


\end{document}
