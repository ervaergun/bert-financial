{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_tokenizer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPMKjLe5EnM9Wx7I3BwJ0Ee"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"fcVxmj4op3xn"},"source":["# We won't need TensorFlow here\n","!pip uninstall -y tensorflow\n","# Install `transformers` from master\n","!pip install git+https://github.com/huggingface/transformers\n","!pip list | grep -E 'transformers|tokenizers'\n","# transformers version at notebook update --- 2.11.0\n","# tokenizers version at notebook update --- 0.8.0rc1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i9SzrLbip78Q"},"source":["%%time \n","from pathlib import Path\n","\n","from tokenizers import BertWordPieceTokenizer\n","\n","#paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n","paths = r\"/content/drive/MyDrive/100mbout/out2006_3.txt\"\n","# Initialize a tokenizer\n","tokenizer = BertWordPieceTokenizer()\n","\n","# Customize training\n","tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2,special_tokens=[\n","                \"[UNK]\",\n","                \"[SEP]\",\n","                \"[PAD]\",\n","                \"[CLS]\",\n","                \"[MASK]\",                                                               \n","\n","              ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3vjMoLAqD66"},"source":["!mkdir bert\n","tokenizer.save_model(\"bert\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlJvH918szsz"},"source":["from tokenizers.implementations import BertWordPieceTokenizer\n","from tokenizers.processors import BertProcessing\n","\n","\n","tokenizer = BertWordPieceTokenizer(\n","    \"./bert/vocab.txt\",\n","    \n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrx7IMXGs0g_"},"source":["tokenizer._tokenizer.post_processor = BertProcessing(\n","    (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n","    (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",")\n","tokenizer.enable_truncation(max_length=512)"],"execution_count":null,"outputs":[]}]}