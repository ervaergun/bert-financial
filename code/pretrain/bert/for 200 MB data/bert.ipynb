{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert.ipynb","provenance":[{"file_id":"1HQvMbRChFkpQOQ8Tsy_NzBlIzSG3vTsV","timestamp":1620595146705},{"file_id":"17m7UA5359alWxaRSR5oDbov6qe2_aMqf","timestamp":1620125613109},{"file_id":"1oXF3aD6qfv8B3fglgxUF7sebZ_XURkOl","timestamp":1619871227592}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1dvbHuobtlhQQiSQ_4AnbxowgcY6aqJxz","authorship_tag":"ABX9TyOQPm1gV0vEIN5HUJ/lnQpd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Pw-zmLjoHBpR"},"source":["#!pip install numpy, pandas\n","!pip uninstall -y tensorflow\n","!pip install sentencepiece\n","#!git clone https://github.com/zihangdai/xlnet\n","# Install `transformers` from master\n","!pip install git+https://github.com/huggingface/transformers\n","!git clone https://github.com/huggingface/transformers.git\n","!pip list | grep -E 'transformers|tokenizers'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQfxYFjoBdCv","executionInfo":{"status":"ok","timestamp":1620371031642,"user_tz":-180,"elapsed":627,"user":{"displayName":"z erva ergün","photoUrl":"","userId":"16255015835614953460"}},"outputId":"a94b84f7-ae8e-422b-add3-feb1f3a57674"},"source":["cd \"/content/transformers/examples/pytorch/language-modeling\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/transformers/examples/pytorch/language-modeling\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M_F8LeyfAAt_","executionInfo":{"status":"ok","timestamp":1620599804007,"user_tz":-180,"elapsed":66310,"user":{"displayName":"z erva ergün","photoUrl":"","userId":"16255015835614953460"}},"outputId":"46537d97-cdf1-45c7-8953-ea6775271ceb"},"source":["%%time \n","from pathlib import Path\n","\n","from tokenizers import BertWordPieceTokenizer\n","\n","#paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n","paths = r\"/content/drive/MyDrive/100mbout/out2006_3.txt\"\n","# Initialize a tokenizer\n","tokenizer = BertWordPieceTokenizer()\n","\n","# Customize training\n","tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2,special_tokens=[\n","                \"[UNK]\",\n","                \"[SEP]\",\n","                \"[PAD]\",\n","                \"[CLS]\",\n","                \"[MASK]\",                                                               \n","\n","              ])"],"execution_count":24,"outputs":[{"output_type":"stream","text":["CPU times: user 3min 40s, sys: 5.07 s, total: 3min 45s\n","Wall time: 1min 5s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wMcWe67YAkbY","executionInfo":{"status":"ok","timestamp":1620599808947,"user_tz":-180,"elapsed":750,"user":{"displayName":"z erva ergün","photoUrl":"","userId":"16255015835614953460"}},"outputId":"4f32ec55-be71-4d93-de6c-725877bebfc7"},"source":["!mkdir bert\n","tokenizer.save_model(\"bert\")"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['bert/vocab.txt']"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"kwIsDLfpAm-M","executionInfo":{"status":"ok","timestamp":1620599810939,"user_tz":-180,"elapsed":721,"user":{"displayName":"z erva ergün","photoUrl":"","userId":"16255015835614953460"}}},"source":["from tokenizers.implementations import BertWordPieceTokenizer\n","from tokenizers.processors import BertProcessing\n","\n","\n","tokenizer = BertWordPieceTokenizer(\n","    \"./bert/vocab.txt\",\n","    \n",")"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"zpj_HmSFAnej","executionInfo":{"status":"ok","timestamp":1620599813068,"user_tz":-180,"elapsed":728,"user":{"displayName":"z erva ergün","photoUrl":"","userId":"16255015835614953460"}}},"source":["tokenizer._tokenizer.post_processor = BertProcessing(\n","    (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n","    (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",")\n","tokenizer.enable_truncation(max_length=512)"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ATU0fngFJrAm"},"source":["import sentencepiece as spm\n","spm.SentencePieceTrainer.Train('--input=/content/out2006.txt  --vocab_size=2362 --model_prefix=sp10m.cased.v3 --character_coverage=0.99995 --model_type=unigram')"]},{"cell_type":"code","metadata":{"id":"wb6Fzs6SF6pp"},"source":["!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2n_M34UNl-v","executionInfo":{"status":"ok","timestamp":1620597337986,"user_tz":-180,"elapsed":627,"user":{"displayName":"z erva ergün","photoUrl":"","userId":"16255015835614953460"}},"outputId":"6212998f-330e-49e5-fea2-eebd7cd191de"},"source":["cd /content"],"execution_count":19,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2IeewMCiAKJ8","executionInfo":{"status":"ok","timestamp":1620634492949,"user_tz":-180,"elapsed":34620716,"user":{"displayName":"z erva ergün","photoUrl":"","userId":"16255015835614953460"}},"outputId":"71aa5186-3e9d-4224-a1fb-f2421f05e135"},"source":["!python /content/transformers/examples/pytorch/language-modeling/run_mlm.py \\\n","    --train_file \"/content/drive/MyDrive/100mbout/out2006_3.txt\" \\\n","    --tokenizer_name \"/content/bert\" \\\n","    --model_type \"bert\" \\\n","    --max_steps 1000 \\\n","    --pad_to_max_length True \\\n","    --do_train \\\n","    --output_dir /content/bert-mlm"],"execution_count":29,"outputs":[{"output_type":"stream","text":["05/09/2021 22:37:53 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n","05/09/2021 22:37:53 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/bert-mlm, overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=1000, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May09_22-37-53_929599ee3bc3, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/bert-mlm, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=0, mp_parameters=)\n","05/09/2021 22:37:53 - WARNING - datasets.builder -   Using custom data configuration default-3dfe598f22dbbfe0\n","05/09/2021 22:37:53 - WARNING - datasets.builder -   Reusing dataset text (/root/.cache/huggingface/datasets/text/default-3dfe598f22dbbfe0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n","05/09/2021 22:37:53 - WARNING - __main__ -   You are instantiating a new config instance from scratch.\n","[INFO|tokenization_utils_base.py:1651] 2021-05-09 22:37:53,723 >> Didn't find file /content/bert/tokenizer.json. We won't load it.\n","[INFO|tokenization_utils_base.py:1651] 2021-05-09 22:37:53,723 >> Didn't find file /content/bert/added_tokens.json. We won't load it.\n","[INFO|tokenization_utils_base.py:1651] 2021-05-09 22:37:53,724 >> Didn't find file /content/bert/special_tokens_map.json. We won't load it.\n","[INFO|tokenization_utils_base.py:1651] 2021-05-09 22:37:53,724 >> Didn't find file /content/bert/tokenizer_config.json. We won't load it.\n","[INFO|tokenization_utils_base.py:1715] 2021-05-09 22:37:53,724 >> loading file /content/bert/vocab.txt\n","[INFO|tokenization_utils_base.py:1715] 2021-05-09 22:37:53,724 >> loading file None\n","[INFO|tokenization_utils_base.py:1715] 2021-05-09 22:37:53,724 >> loading file None\n","[INFO|tokenization_utils_base.py:1715] 2021-05-09 22:37:53,724 >> loading file None\n","[INFO|tokenization_utils_base.py:1715] 2021-05-09 22:37:53,724 >> loading file None\n","05/09/2021 22:37:53 - INFO - __main__ -   Training new model from scratch\n","  0% 0/1030 [00:00<?, ?ba/s][WARNING|tokenization_utils_base.py:3171] 2021-05-09 22:37:57,206 >> Token indices sequence length is longer than the specified maximum sequence length for this model (813 > 512). Running this sequence through the model will result in indexing errors\n","100% 1030/1030 [03:01<00:00,  5.67ba/s]\n","100% 1030/1030 [14:06<00:00,  1.22ba/s]\n","[INFO|trainer.py:397] 2021-05-09 22:55:05,772 >> max_steps is given, it will override any value given in num_train_epochs\n","[INFO|trainer.py:515] 2021-05-09 22:55:05,926 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n","[INFO|trainer.py:1144] 2021-05-09 22:55:05,969 >> ***** Running training *****\n","[INFO|trainer.py:1145] 2021-05-09 22:55:05,969 >>   Num examples = 136051\n","[INFO|trainer.py:1146] 2021-05-09 22:55:05,969 >>   Num Epochs = 1\n","[INFO|trainer.py:1147] 2021-05-09 22:55:05,969 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:1148] 2021-05-09 22:55:05,969 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:1149] 2021-05-09 22:55:05,969 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1150] 2021-05-09 22:55:05,969 >>   Total optimization steps = 1000\n","{'loss': 6.0284, 'learning_rate': 2.5e-05, 'epoch': 0.03}\n"," 50% 500/1000 [4:40:53<4:39:39, 33.56s/it][INFO|trainer.py:1867] 2021-05-10 03:35:59,932 >> Saving model checkpoint to /content/bert-mlm/checkpoint-500\n","[INFO|configuration_utils.py:351] 2021-05-10 03:35:59,939 >> Configuration saved in /content/bert-mlm/checkpoint-500/config.json\n","[INFO|modeling_utils.py:883] 2021-05-10 03:36:00,859 >> Model weights saved in /content/bert-mlm/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1924] 2021-05-10 03:36:00,860 >> tokenizer config file saved in /content/bert-mlm/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1930] 2021-05-10 03:36:00,861 >> Special tokens file saved in /content/bert-mlm/checkpoint-500/special_tokens_map.json\n","{'loss': 5.3087, 'learning_rate': 0.0, 'epoch': 0.06}\n","100% 1000/1000 [9:19:39<00:00, 33.38s/it][INFO|trainer.py:1867] 2021-05-10 08:14:45,698 >> Saving model checkpoint to /content/bert-mlm/checkpoint-1000\n","[INFO|configuration_utils.py:351] 2021-05-10 08:14:45,699 >> Configuration saved in /content/bert-mlm/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:883] 2021-05-10 08:14:46,709 >> Model weights saved in /content/bert-mlm/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1924] 2021-05-10 08:14:46,710 >> tokenizer config file saved in /content/bert-mlm/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1930] 2021-05-10 08:14:46,711 >> Special tokens file saved in /content/bert-mlm/checkpoint-1000/special_tokens_map.json\n","[INFO|trainer.py:1340] 2021-05-10 08:14:49,275 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 33583.3054, 'train_samples_per_second': 0.03, 'epoch': 0.06}\n","100% 1000/1000 [9:19:43<00:00, 33.58s/it]\n","[INFO|trainer.py:1867] 2021-05-10 08:14:49,591 >> Saving model checkpoint to /content/bert-mlm\n","[INFO|configuration_utils.py:351] 2021-05-10 08:14:49,592 >> Configuration saved in /content/bert-mlm/config.json\n","[INFO|modeling_utils.py:883] 2021-05-10 08:14:50,768 >> Model weights saved in /content/bert-mlm/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:1924] 2021-05-10 08:14:50,769 >> tokenizer config file saved in /content/bert-mlm/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1930] 2021-05-10 08:14:50,769 >> Special tokens file saved in /content/bert-mlm/special_tokens_map.json\n","[INFO|trainer_pt_utils.py:907] 2021-05-10 08:14:50,863 >> ***** train metrics *****\n","[INFO|trainer_pt_utils.py:912] 2021-05-10 08:14:50,863 >>   epoch                      =       0.06\n","[INFO|trainer_pt_utils.py:912] 2021-05-10 08:14:50,863 >>   init_mem_cpu_alloc_delta   =        0MB\n","[INFO|trainer_pt_utils.py:912] 2021-05-10 08:14:50,864 >>   init_mem_cpu_peaked_delta  =        0MB\n","[INFO|trainer_pt_utils.py:912] 2021-05-10 08:14:50,864 >>   train_mem_cpu_alloc_delta  =     7813MB\n","[INFO|trainer_pt_utils.py:912] 2021-05-10 08:14:50,864 >>   train_mem_cpu_peaked_delta =      692MB\n","[INFO|trainer_pt_utils.py:912] 2021-05-10 08:14:50,864 >>   train_runtime              = 9:19:43.30\n","[INFO|trainer_pt_utils.py:912] 2021-05-10 08:14:50,864 >>   train_samples              =     136051\n","[INFO|trainer_pt_utils.py:912] 2021-05-10 08:14:50,864 >>   train_samples_per_second   =       0.03\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43qJEl-IBgfY","executionInfo":{"status":"ok","timestamp":1620450758155,"user_tz":-180,"elapsed":137817,"user":{"displayName":"z erva ergün","photoUrl":"","userId":"16255015835614953460"}},"outputId":"e9770405-e6e7-4f48-b954-c715a0f859ec"},"source":["!python /content/transformers/examples/pytorch/language-modeling/run_plm.py \\\n","    --train_file \"/content/deneme.txt\" \\\n","    --tokenizer_name \"/content/xlnet\" \\\n","    --do_train \\\n","    --max_steps 1000 \\\n","    --resume_from_checkpoint /content/output_xlnet3 \\\n","    --evaluation_strategy steps \\\n","    --output_dir /content/output_xlnet4"],"execution_count":null,"outputs":[{"output_type":"stream","text":["05/08/2021 05:10:22 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n","05/08/2021 05:10:22 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/output_xlnet4, overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=1000, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May08_05-10-22_6b8a03778108, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/output_xlnet4, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=/content/output_xlnet3, _n_gpu=0, mp_parameters=)\n","05/08/2021 05:10:22 - WARNING - datasets.builder -   Using custom data configuration default-132a2d3bfc98849a\n","05/08/2021 05:10:22 - WARNING - datasets.builder -   Reusing dataset text (/root/.cache/huggingface/datasets/text/default-132a2d3bfc98849a/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n","05/08/2021 05:10:22 - WARNING - __main__ -   You are instantiating a new config instance from scratch.\n","05/08/2021 05:10:22 - INFO - __main__ -   Training new model from scratch\n","100% 1/1 [00:00<00:00, 154.40ba/s]\n","100% 1/1 [00:00<00:00, 317.89ba/s]\n","[INFO|trainer.py:397] 2021-05-08 05:10:32,815 >> max_steps is given, it will override any value given in num_train_epochs\n","[INFO|trainer.py:1042] 2021-05-08 05:10:32,949 >> Loading model from /content/output_xlnet3).\n","[INFO|trainer.py:1144] 2021-05-08 05:10:45,295 >> ***** Running training *****\n","[INFO|trainer.py:1145] 2021-05-08 05:10:45,295 >>   Num examples = 1\n","[INFO|trainer.py:1146] 2021-05-08 05:10:45,295 >>   Num Epochs = 1000\n","[INFO|trainer.py:1147] 2021-05-08 05:10:45,295 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:1148] 2021-05-08 05:10:45,295 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:1149] 2021-05-08 05:10:45,295 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1150] 2021-05-08 05:10:45,295 >>   Total optimization steps = 1000\n","[INFO|trainer.py:1170] 2021-05-08 05:10:45,300 >>   Continuing training from checkpoint, will skip to saved global_step\n","[INFO|trainer.py:1171] 2021-05-08 05:10:45,300 >>   Continuing training from epoch 1\n","[INFO|trainer.py:1172] 2021-05-08 05:10:45,300 >>   Continuing training from global step 1\n","[INFO|trainer.py:1175] 2021-05-08 05:10:45,300 >>   Will skip the first 1 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n","Skipping the first batches: : 0it [00:00, ?it/s]\n","  0% 0/1000 [00:00<?, ?it/s]\u001b[A\n","Skipping the first batches: : 0it [00:00, ?it/s]\n","  0% 2/1000 [01:20<11:05:43, 40.02s/it]^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EmExdL2wewDd","executionInfo":{"status":"ok","timestamp":1620636923665,"user_tz":-180,"elapsed":970,"user":{"displayName":"z erva ergün","photoUrl":"","userId":"16255015835614953460"}},"outputId":"840d2692-5c95-4755-a156-f18af8ec687e"},"source":["cd /content/bert-mlm"],"execution_count":30,"outputs":[{"output_type":"stream","text":["/content/bert-mlm\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iJA6QmXyftQc","executionInfo":{"status":"ok","timestamp":1620637049937,"user_tz":-180,"elapsed":612,"user":{"displayName":"z erva ergün","photoUrl":"","userId":"16255015835614953460"}},"outputId":"a067ed75-64dd-4cb1-fe03-c01dc952bd6e"},"source":["cd .."],"execution_count":32,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_3YEOqnfPLc","executionInfo":{"status":"ok","timestamp":1620636983694,"user_tz":-180,"elapsed":3879,"user":{"displayName":"z erva ergün","photoUrl":"","userId":"16255015835614953460"}},"outputId":"9e6bb8c6-a9b3-4a07-bc93-37db948da5b8"},"source":["!sudo apt-get install zip"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","zip is already the newest version (3.0-11build1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'sudo apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0x1jIIDnfdH5","executionInfo":{"status":"ok","timestamp":1620637239073,"user_tz":-180,"elapsed":144391,"user":{"displayName":"z erva ergün","photoUrl":"","userId":"16255015835614953460"}},"outputId":"34d1c457-6d6e-411d-c9d2-cb39f640733d"},"source":["!zip -r bert-mlm.zip bert-mlm"],"execution_count":33,"outputs":[{"output_type":"stream","text":["  adding: bert-mlm/ (stored 0%)\n","  adding: bert-mlm/special_tokens_map.json (deflated 40%)\n","  adding: bert-mlm/trainer_state.json (deflated 57%)\n","  adding: bert-mlm/checkpoint-500/ (stored 0%)\n","  adding: bert-mlm/checkpoint-500/special_tokens_map.json (deflated 40%)\n","  adding: bert-mlm/checkpoint-500/trainer_state.json (deflated 46%)\n","  adding: bert-mlm/checkpoint-500/tokenizer_config.json (deflated 42%)\n","  adding: bert-mlm/checkpoint-500/config.json (deflated 46%)\n","  adding: bert-mlm/checkpoint-500/pytorch_model.bin (deflated 7%)\n","  adding: bert-mlm/checkpoint-500/training_args.bin (deflated 46%)\n","  adding: bert-mlm/checkpoint-500/vocab.txt (deflated 45%)\n","  adding: bert-mlm/checkpoint-500/rng_state.pth (deflated 23%)\n","  adding: bert-mlm/checkpoint-500/scheduler.pt (deflated 50%)\n","  adding: bert-mlm/checkpoint-500/optimizer.pt (deflated 8%)\n","  adding: bert-mlm/checkpoint-500/tokenizer.json (deflated 58%)\n","  adding: bert-mlm/tokenizer_config.json (deflated 42%)\n","  adding: bert-mlm/config.json (deflated 46%)\n","  adding: bert-mlm/pytorch_model.bin (deflated 7%)\n","  adding: bert-mlm/all_results.json (deflated 50%)\n","  adding: bert-mlm/training_args.bin (deflated 46%)\n","  adding: bert-mlm/vocab.txt (deflated 45%)\n","  adding: bert-mlm/checkpoint-1000/ (stored 0%)\n","  adding: bert-mlm/checkpoint-1000/special_tokens_map.json (deflated 40%)\n","  adding: bert-mlm/checkpoint-1000/trainer_state.json (deflated 52%)\n","  adding: bert-mlm/checkpoint-1000/tokenizer_config.json (deflated 42%)\n","  adding: bert-mlm/checkpoint-1000/config.json (deflated 46%)\n","  adding: bert-mlm/checkpoint-1000/pytorch_model.bin (deflated 7%)\n","  adding: bert-mlm/checkpoint-1000/training_args.bin (deflated 46%)\n","  adding: bert-mlm/checkpoint-1000/vocab.txt (deflated 45%)\n","  adding: bert-mlm/checkpoint-1000/rng_state.pth (deflated 23%)\n","  adding: bert-mlm/checkpoint-1000/scheduler.pt (deflated 50%)\n","  adding: bert-mlm/checkpoint-1000/optimizer.pt (deflated 8%)\n","  adding: bert-mlm/checkpoint-1000/tokenizer.json (deflated 58%)\n","  adding: bert-mlm/tokenizer.json (deflated 58%)\n","  adding: bert-mlm/train_results.json (deflated 50%)\n"],"name":"stdout"}]}]}